{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6938728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from github import Github, Auth\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"Specify GITHUB_TOKEN in .env file.\")\n",
    "\n",
    "g = Github(auth=Auth.Token(GITHUB_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0683cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'AlexIoannides/pyspark-example-project', 'url': 'https://github.com/AlexIoannides/pyspark-example-project.git', 'stars': 2013, 'description': 'Implementing best practices for PySpark ETL jobs and applications.'}\n",
      "{'name': 'uber/petastorm', 'url': 'https://github.com/uber/petastorm.git', 'stars': 1863, 'description': 'Petastorm library enables single machine or distributed training and evaluation of deep learning models from datasets in Apache Parquet format. It supports ML frameworks such as Tensorflow, Pytorch, and PySpark and can be used from pure Python code.'}\n",
      "{'name': 'jadianes/spark-py-notebooks', 'url': 'https://github.com/jadianes/spark-py-notebooks.git', 'stars': 1666, 'description': 'Apache Spark & Python (pySpark) tutorials for Big Data Analysis and Machine Learning as IPython / Jupyter notebooks'}\n"
     ]
    }
   ],
   "source": [
    "query = \"pyspark in:name,description\"\n",
    "sort = \"stars\"\n",
    "order = \"desc\"\n",
    "limit = 3\n",
    "\n",
    "repos = g.search_repositories(query=query, sort=sort, order=order)\n",
    "\n",
    "popular_repos = []\n",
    "for i, repo in enumerate(repos):\n",
    "    if i >= limit:\n",
    "        break\n",
    "\n",
    "    popular_repos.append({\n",
    "        \"name\": repo.full_name,\n",
    "        \"url\": repo.clone_url,\n",
    "        \"stars\": repo.stargazers_count,\n",
    "        \"description\": repo.description\n",
    "    })\n",
    "\n",
    "for repo in popular_repos:\n",
    "    print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff7305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 potential matches in AlexIoannides/pyspark-example-project\n",
      "\n",
      "Found 64 potential matches in uber/petastorm\n",
      "\n",
      "Found 31 potential matches in jadianes/spark-py-notebooks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import json\n",
    "from nbconvert import PythonExporter\n",
    "import nbformat\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "# Create results jsonl\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "summary_path = os.path.join(\"results\", \"all_results.jsonl\")\n",
    "\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as summary_file:\n",
    "    for repo in popular_repos:\n",
    "        repo_name = repo[\"name\"]\n",
    "        clone_url = repo[\"url\"]\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            # clone repo into temporary directory\n",
    "            repo_dir = os.path.join(tmpdir, \"repo\")\n",
    "            git.Repo.clone_from(clone_url, repo_dir, depth=1)\n",
    "\n",
    "            # find .ipynb files and convert into .py files\n",
    "            for root, _, files in os.walk(repo_dir):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".ipynb\"):\n",
    "                        ipynb_path = os.path.join(root, file)\n",
    "                        py_path = os.path.join(root, file.replace(\".ipynb\", \".py\"))\n",
    "\n",
    "                        with open(ipynb_path, \"r\", encoding=\"utf-8\") as ipynbf:\n",
    "                            nb_node = nbformat.read(ipynbf, as_version=4)\n",
    "\n",
    "                            exporter = PythonExporter()\n",
    "                            python_code, _ = exporter.from_notebook_node(nb_node)\n",
    "\n",
    "                            with open(py_path, \"w\", encoding=\"utf-8\") as pyf:\n",
    "                                pyf.write(python_code)\n",
    "\n",
    "            # use semgrep to detect PySpark sql, df, udf usage\n",
    "            result = subprocess.run(\n",
    "                [\"semgrep\", \"scan\", \"--config\", \"pyspark-rules.yml\", repo_dir, \"--json\"],\n",
    "                capture_output=True,\n",
    "                encoding=\"utf-8\", #quick fix: to avoid byte serialization issue on Windows laptops.\n",
    "                text=True,\n",
    "                check=False\n",
    "            )\n",
    "\n",
    "            # parse matches\n",
    "            try:\n",
    "                data = json.loads(result.stdout)\n",
    "                matches = data.get(\"results\", [])\n",
    "                print(f\"Found {len(matches)} potential matches in {repo_name}\\n\")\n",
    "\n",
    "                repo_results = {\n",
    "                    \"repo_name\" : repo_name,\n",
    "                    \"clone_url\" : clone_url,\n",
    "                    \"matches\" : []\n",
    "                }\n",
    "                for match in matches:\n",
    "                    file_path = match[\"path\"]\n",
    "                    start_offset = match[\"start\"][\"offset\"]\n",
    "                    end_offset = match[\"end\"][\"offset\"]\n",
    "\n",
    "                    with open(file_path, \"r\") as f:\n",
    "                        content = f.read()\n",
    "                        snippet = content[start_offset:end_offset]\n",
    "\n",
    "                        repo_results[\"matches\"].append({\n",
    "                            \"file\": os.path.relpath(file_path, tmpdir),\n",
    "                            \"message\": match[\"extra\"].get(\"message\", \"\"),\n",
    "                            \"start_offset\": start_offset,\n",
    "                            \"end_offset\": end_offset,\n",
    "                            \"snippet\": snippet.strip()\n",
    "                        })\n",
    "\n",
    "                # write to results jsonl\n",
    "                summary_file.write(json.dumps(repo_results, ensure_ascii=False) + \"\\n\")\n",
    "                summary_file.flush()\n",
    "\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Semgrep output not valid JSON.\")\n",
    "                print(result.stdout[:500])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
