{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6938728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from github import Github, Auth\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"Specify GITHUB_TOKEN in .env file.\")\n",
    "\n",
    "g = Github(auth=Auth.Token(GITHUB_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0683cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'AlexIoannides/pyspark-example-project', 'url': 'https://github.com/AlexIoannides/pyspark-example-project.git', 'stars': 2013, 'description': 'Implementing best practices for PySpark ETL jobs and applications.'}\n",
      "{'name': 'uber/petastorm', 'url': 'https://github.com/uber/petastorm.git', 'stars': 1863, 'description': 'Petastorm library enables single machine or distributed training and evaluation of deep learning models from datasets in Apache Parquet format. It supports ML frameworks such as Tensorflow, Pytorch, and PySpark and can be used from pure Python code.'}\n",
      "{'name': 'jadianes/spark-py-notebooks', 'url': 'https://github.com/jadianes/spark-py-notebooks.git', 'stars': 1666, 'description': 'Apache Spark & Python (pySpark) tutorials for Big Data Analysis and Machine Learning as IPython / Jupyter notebooks'}\n"
     ]
    }
   ],
   "source": [
    "query = \"pyspark in:name,description\"\n",
    "sort = \"stars\"\n",
    "order = \"desc\"\n",
    "limit = 3\n",
    "\n",
    "repos = g.search_repositories(query=query, sort=sort, order=order)\n",
    "\n",
    "popular_repos = []\n",
    "for i, repo in enumerate(repos):\n",
    "    if i >= limit:\n",
    "        break\n",
    "\n",
    "    popular_repos.append({\n",
    "        \"name\": repo.full_name,\n",
    "        \"url\": repo.clone_url,\n",
    "        \"stars\": repo.stargazers_count,\n",
    "        \"description\": repo.description\n",
    "    })\n",
    "\n",
    "for repo in popular_repos:\n",
    "    print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53ff7305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': '1.140.0', 'results': [{'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', 'start': {'line': 116, 'col': 1, 'offset': 4469}, 'end': {'line': 116, 'col': 65, 'offset': 4533}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', 'start': {'line': 116, 'col': 1, 'offset': 4469}, 'end': {'line': 116, 'col': 90, 'offset': 4558}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', 'start': {'line': 128, 'col': 1, 'offset': 4878}, 'end': {'line': 128, 'col': 65, 'offset': 4942}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', 'start': {'line': 128, 'col': 1, 'offset': 4878}, 'end': {'line': 128, 'col': 103, 'offset': 4980}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', 'start': {'line': 128, 'col': 1, 'offset': 4878}, 'end': {'line': 128, 'col': 140, 'offset': 5017}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', 'start': {'line': 128, 'col': 1, 'offset': 4878}, 'end': {'line': 128, 'col': 165, 'offset': 5042}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', 'start': {'line': 166, 'col': 1, 'offset': 6016}, 'end': {'line': 166, 'col': 40, 'offset': 6055}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', 'start': {'line': 166, 'col': 1, 'offset': 6016}, 'end': {'line': 166, 'col': 57, 'offset': 6072}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', 'start': {'line': 178, 'col': 1, 'offset': 6351}, 'end': {'line': 178, 'col': 57, 'offset': 6407}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', 'start': {'line': 178, 'col': 1, 'offset': 6351}, 'end': {'line': 178, 'col': 91, 'offset': 6441}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', 'start': {'line': 192, 'col': 1, 'offset': 6823}, 'end': {'line': 192, 'col': 70, 'offset': 6892}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', 'start': {'line': 192, 'col': 1, 'offset': 6823}, 'end': {'line': 192, 'col': 142, 'offset': 6964}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb2-rdd-basics/nb2-rdd-basics.py', 'start': {'line': 39, 'col': 19, 'offset': 1468}, 'end': {'line': 39, 'col': 60, 'offset': 1509}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb2-rdd-basics/nb2-rdd-basics.py', 'start': {'line': 141, 'col': 27, 'offset': 5234}, 'end': {'line': 141, 'col': 75, 'offset': 5282}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb3-rdd-sampling/nb3-rdd-sampling.py', 'start': {'line': 60, 'col': 22, 'offset': 2680}, 'end': {'line': 60, 'col': 76, 'offset': 2734}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb3-rdd-sampling/nb3-rdd-sampling.py', 'start': {'line': 79, 'col': 15, 'offset': 3227}, 'end': {'line': 79, 'col': 62, 'offset': 3274}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb4-rdd-set/nb4-rdd-set.py', 'start': {'line': 37, 'col': 19, 'offset': 1493}, 'end': {'line': 37, 'col': 60, 'offset': 1534}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb4-rdd-set/nb4-rdd-set.py', 'start': {'line': 103, 'col': 13, 'offset': 3319}, 'end': {'line': 103, 'col': 52, 'offset': 3358}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb4-rdd-set/nb4-rdd-set.py', 'start': {'line': 112, 'col': 12, 'offset': 3442}, 'end': {'line': 112, 'col': 51, 'offset': 3481}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb5-rdd-aggregations/nb5-rdd-aggregations.py', 'start': {'line': 41, 'col': 19, 'offset': 1553}, 'end': {'line': 41, 'col': 62, 'offset': 1596}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb5-rdd-aggregations/nb5-rdd-aggregations.py', 'start': {'line': 42, 'col': 19, 'offset': 1615}, 'end': {'line': 42, 'col': 62, 'offset': 1658}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb7-mllib-statistics/nb7-mllib-statistics.py', 'start': {'line': 101, 'col': 21, 'offset': 5095}, 'end': {'line': 101, 'col': 72, 'offset': 5146}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb7-mllib-statistics/nb7-mllib-statistics.py', 'start': {'line': 134, 'col': 25, 'offset': 6385}, 'end': {'line': 134, 'col': 95, 'offset': 6455}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb8-mllib-logit/nb8-mllib-logit.py', 'start': {'line': 128, 'col': 17, 'offset': 4481}, 'end': {'line': 128, 'col': 63, 'offset': 4527}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb8-mllib-logit/nb8-mllib-logit.py', 'start': {'line': 201, 'col': 17, 'offset': 9049}, 'end': {'line': 201, 'col': 63, 'offset': 9095}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb8-mllib-logit/nb8-mllib-logit.py', 'start': {'line': 309, 'col': 17, 'offset': 13832}, 'end': {'line': 309, 'col': 63, 'offset': 13878}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb9-mllib-trees/nb9-mllib-trees.py', 'start': {'line': 89, 'col': 13, 'offset': 3899}, 'end': {'line': 89, 'col': 52, 'offset': 3938}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb9-mllib-trees/nb9-mllib-trees.py', 'start': {'line': 90, 'col': 12, 'offset': 3960}, 'end': {'line': 90, 'col': 51, 'offset': 3999}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb9-mllib-trees/nb9-mllib-trees.py', 'start': {'line': 91, 'col': 9, 'offset': 4018}, 'end': {'line': 91, 'col': 48, 'offset': 4057}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb9-mllib-trees/nb9-mllib-trees.py', 'start': {'line': 169, 'col': 17, 'offset': 6753}, 'end': {'line': 169, 'col': 63, 'offset': 6799}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'pyspark-df-detection', 'path': '/tmp/tmpngz4j9yj/repo/nb9-mllib-trees/nb9-mllib-trees.py', 'start': {'line': 266, 'col': 17, 'offset': 10802}, 'end': {'line': 266, 'col': 71, 'offset': 10856}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}], 'errors': [], 'paths': {'scanned': ['/tmp/tmpngz4j9yj/repo/nb1-rdd-creation/nb1-rdd-creation.py', '/tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py', '/tmp/tmpngz4j9yj/repo/nb2-rdd-basics/nb2-rdd-basics.py', '/tmp/tmpngz4j9yj/repo/nb3-rdd-sampling/nb3-rdd-sampling.py', '/tmp/tmpngz4j9yj/repo/nb4-rdd-set/nb4-rdd-set.py', '/tmp/tmpngz4j9yj/repo/nb5-rdd-aggregations/nb5-rdd-aggregations.py', '/tmp/tmpngz4j9yj/repo/nb6-rdd-key-value/nb6-rdd-key-value.py', '/tmp/tmpngz4j9yj/repo/nb7-mllib-statistics/nb7-mllib-statistics.py', '/tmp/tmpngz4j9yj/repo/nb8-mllib-logit/nb8-mllib-logit.py', '/tmp/tmpngz4j9yj/repo/nb9-mllib-trees/nb9-mllib-trees.py']}, 'time': {'rules': [], 'rules_parse_time': 0.007483959197998047, 'profiling_times': {'config_time': 0.34499359130859375, 'core_time': 0.9222438335418701, 'ignores_time': 0.00016045570373535156, 'total_time': 1.2678794860839844}, 'parsing_time': {'total_time': 0.0, 'per_file_time': {'mean': 0.0, 'std_dev': 0.0}, 'very_slow_stats': {'time_ratio': 0.0, 'count_ratio': 0.0}, 'very_slow_files': []}, 'scanning_time': {'total_time': 0.45808935165405273, 'per_file_time': {'mean': 0.04580893516540528, 'std_dev': 0.00024383080784900823}, 'very_slow_stats': {'time_ratio': 0.0, 'count_ratio': 0.0}, 'very_slow_files': []}, 'matching_time': {'total_time': 0.0, 'per_file_and_rule_time': {'mean': 0.0, 'std_dev': 0.0}, 'very_slow_stats': {'time_ratio': 0.0, 'count_ratio': 0.0}, 'very_slow_rules_on_files': []}, 'tainting_time': {'total_time': 0.0, 'per_def_and_rule_time': {'mean': 0.0, 'std_dev': 0.0}, 'very_slow_stats': {'time_ratio': 0.0, 'count_ratio': 0.0}, 'very_slow_rules_on_defs': []}, 'fixpoint_timeouts': [], 'prefiltering': {'project_level_time': 0.0, 'file_level_time': 0.0, 'rules_with_project_prefilters_ratio': 0.0, 'rules_with_file_prefilters_ratio': 1.0, 'rules_selected_ratio': 0.3333333333333333, 'rules_matched_ratio': 0.3333333333333333}, 'targets': [], 'total_bytes': 0, 'max_memory_bytes': 77630016}, 'engine_requested': 'OSS', 'skipped_rules': []}\n",
      "Found 31 potential UDF matches\n",
      "\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py\n",
      "Detected PySpark DF operation.\n",
      "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py\n",
      "Detected PySpark DF operation.\n",
      "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").groupBy(\"protocol_type\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py\n",
      "Detected PySpark DF operation.\n",
      "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py\n",
      "Detected PySpark DF operation.\n",
      "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").filter(interactions_df.duration>1000)\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py\n",
      "Detected PySpark DF operation.\n",
      "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").filter(interactions_df.duration>1000).filter(interactions_df.dst_bytes==0)\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py\n",
      "Detected PySpark DF operation.\n",
      "interactions_df.select(\"protocol_type\", \"duration\", \"dst_bytes\").filter(interactions_df.duration>1000).filter(interactions_df.dst_bytes==0).groupBy(\"protocol_type\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py\n",
      "Detected PySpark DF operation.\n",
      "interactions_labeled_df.select(\"label\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py\n",
      "Detected PySpark DF operation.\n",
      "interactions_labeled_df.select(\"label\").groupBy(\"label\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py\n",
      "Detected PySpark DF operation.\n",
      "interactions_labeled_df.select(\"label\", \"protocol_type\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py\n",
      "Detected PySpark DF operation.\n",
      "interactions_labeled_df.select(\"label\", \"protocol_type\").groupBy(\"label\", \"protocol_type\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py\n",
      "Detected PySpark DF operation.\n",
      "interactions_labeled_df.select(\"label\", \"protocol_type\", \"dst_bytes\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb10-sql-dataframes/nb10-sql-dataframes.py\n",
      "Detected PySpark DF operation.\n",
      "interactions_labeled_df.select(\"label\", \"protocol_type\", \"dst_bytes\").groupBy(\"label\", \"protocol_type\", interactions_labeled_df.dst_bytes==0)\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb2-rdd-basics/nb2-rdd-basics.py\n",
      "Detected PySpark DF operation.\n",
      "raw_data.filter(lambda x: 'normal.' in x)\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb2-rdd-basics/nb2-rdd-basics.py\n",
      "Detected PySpark DF operation.\n",
      "key_csv_data.filter(lambda x: x[0] == \"normal.\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb3-rdd-sampling/nb3-rdd-sampling.py\n",
      "Detected PySpark DF operation.\n",
      "raw_data_sample_items.filter(lambda x: \"normal.\" in x)\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb3-rdd-sampling/nb3-rdd-sampling.py\n",
      "Detected PySpark DF operation.\n",
      "raw_data_items.filter(lambda x: \"normal.\" in x)\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb4-rdd-set/nb4-rdd-set.py\n",
      "Detected PySpark DF operation.\n",
      "raw_data.filter(lambda x: \"normal.\" in x)\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb4-rdd-set/nb4-rdd-set.py\n",
      "Detected PySpark DF operation.\n",
      "csv_data.map(lambda x: x[1]).distinct()\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb4-rdd-set/nb4-rdd-set.py\n",
      "Detected PySpark DF operation.\n",
      "csv_data.map(lambda x: x[2]).distinct()\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb5-rdd-aggregations/nb5-rdd-aggregations.py\n",
      "Detected PySpark DF operation.\n",
      "csv_data.filter(lambda x: x[41]==\"normal.\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb5-rdd-aggregations/nb5-rdd-aggregations.py\n",
      "Detected PySpark DF operation.\n",
      "csv_data.filter(lambda x: x[41]!=\"normal.\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb7-mllib-statistics/nb7-mllib-statistics.py\n",
      "Detected PySpark DF operation.\n",
      "label_vector_data.filter(lambda x: x[0]==\"normal.\")\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb7-mllib-statistics/nb7-mllib-statistics.py\n",
      "Detected PySpark DF operation.\n",
      "raw_data.map(parse_interaction_with_key).filter(lambda x: x[0]==label)\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb8-mllib-logit/nb8-mllib-logit.py\n",
      "Detected PySpark DF operation.\n",
      "labels_and_preds.filter(lambda (v, p): v == p)\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb8-mllib-logit/nb8-mllib-logit.py\n",
      "Detected PySpark DF operation.\n",
      "labels_and_preds.filter(lambda (v, p): v == p)\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb8-mllib-logit/nb8-mllib-logit.py\n",
      "Detected PySpark DF operation.\n",
      "bels_and_preds.filter(lambda (v, p): v == p).c\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb9-mllib-trees/nb9-mllib-trees.py\n",
      "Detected PySpark DF operation.\n",
      "csv_data.map(lambda x: x[1]).distinct()\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb9-mllib-trees/nb9-mllib-trees.py\n",
      "Detected PySpark DF operation.\n",
      "csv_data.map(lambda x: x[2]).distinct()\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb9-mllib-trees/nb9-mllib-trees.py\n",
      "Detected PySpark DF operation.\n",
      "csv_data.map(lambda x: x[3]).distinct()\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb9-mllib-trees/nb9-mllib-trees.py\n",
      "Detected PySpark DF operation.\n",
      "labels_and_preds.filter(lambda (v, p): v == p)\n",
      "====================\n",
      "File:  /tmp/tmpngz4j9yj/repo/nb9-mllib-trees/nb9-mllib-trees.py\n",
      "Detected PySpark DF operation.\n",
      "labels_and_preds_minimal.filter(lambda (v, p): v == p)\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import json\n",
    "from nbconvert import PythonExporter\n",
    "import nbformat\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "for repo in popular_repos[2:]:\n",
    "    clone_url = repo[\"url\"]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        # clone repo into temporary directory\n",
    "        repo_dir = os.path.join(tmpdir, \"repo\")\n",
    "        git.Repo.clone_from(clone_url, repo_dir, depth=1)\n",
    "\n",
    "        # find .ipynb files and convert into .py files\n",
    "        for root, _, files in os.walk(repo_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".ipynb\"):\n",
    "                    ipynb_path = os.path.join(root, file)\n",
    "                    py_path = os.path.join(root, file.replace(\".ipynb\", \".py\"))\n",
    "\n",
    "                    with open(ipynb_path, \"r\", encoding=\"utf-8\") as rf:\n",
    "                        nb_node = nbformat.read(rf, as_version=4)\n",
    "\n",
    "                        exporter = PythonExporter()\n",
    "                        python_code, _ = exporter.from_notebook_node(nb_node)\n",
    "\n",
    "                        with open(py_path, \"w\", encoding=\"utf-8\") as wf:\n",
    "                            wf.write(python_code)\n",
    "\n",
    "        # use semgrep to detect PySpark sql, df, udf usage\n",
    "        result = subprocess.run(\n",
    "            [\"semgrep\", \"scan\", \"--config\", \"pyspark-rules.yml\", repo_dir, \"--json\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=False\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            data = json.loads(result.stdout)\n",
    "            print(data)\n",
    "            matches = data.get(\"results\", [])\n",
    "            print(f\"Found {len(matches)} potential UDF matches\\n\")\n",
    "\n",
    "            for match in matches:\n",
    "                file_path = match[\"path\"]\n",
    "                start_offset = match[\"start\"][\"offset\"]\n",
    "                end_offset = match[\"end\"][\"offset\"]\n",
    "\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                snippet = content[start_offset:end_offset]\n",
    "\n",
    "                print(\"File: \", file_path)\n",
    "                print(match[\"extra\"][\"message\"])\n",
    "                print(snippet)\n",
    "                print(\"=\"*20)\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Semgrep output not valid JSON.\")\n",
    "            print(result.stdout[:500])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
