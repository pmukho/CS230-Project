{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6938728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from github import Github, Auth\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"Specify GITHUB_TOKEN in .env file.\")\n",
    "\n",
    "g = Github(auth=Auth.Token(GITHUB_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0683cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'AlexIoannides/pyspark-example-project', 'url': 'https://github.com/AlexIoannides/pyspark-example-project.git', 'stars': 2013, 'description': 'Implementing best practices for PySpark ETL jobs and applications.'}\n",
      "{'name': 'uber/petastorm', 'url': 'https://github.com/uber/petastorm.git', 'stars': 1865, 'description': 'Petastorm library enables single machine or distributed training and evaluation of deep learning models from datasets in Apache Parquet format. It supports ML frameworks such as Tensorflow, Pytorch, and PySpark and can be used from pure Python code.'}\n",
      "{'name': 'jadianes/spark-py-notebooks', 'url': 'https://github.com/jadianes/spark-py-notebooks.git', 'stars': 1666, 'description': 'Apache Spark & Python (pySpark) tutorials for Big Data Analysis and Machine Learning as IPython / Jupyter notebooks'}\n",
      "{'name': 'ptyadana/SQL-Data-Analysis-and-Visualization-Projects', 'url': 'https://github.com/ptyadana/SQL-Data-Analysis-and-Visualization-Projects.git', 'stars': 1578, 'description': 'SQL data analysis & visualization projects using MySQL, PostgreSQL, SQLite, Tableau, Apache Spark and pySpark.'}\n",
      "{'name': 'hi-primus/optimus', 'url': 'https://github.com/hi-primus/optimus.git', 'stars': 1523, 'description': ':truck: Agile Data Preparation Workflows made\\xa0easy with Pandas, Dask, cuDF, Dask-cuDF, Vaex and PySpark'}\n",
      "{'name': 'spark-examples/pyspark-examples', 'url': 'https://github.com/spark-examples/pyspark-examples.git', 'stars': 1315, 'description': 'Pyspark RDD, DataFrame and Dataset Examples in Python language'}\n",
      "{'name': 'mahmoudparsian/pyspark-tutorial', 'url': 'https://github.com/mahmoudparsian/pyspark-tutorial.git', 'stars': 1256, 'description': 'PySpark-Tutorial provides basic algorithms using PySpark'}\n",
      "{'name': 'palantir/pyspark-style-guide', 'url': 'https://github.com/palantir/pyspark-style-guide.git', 'stars': 1184, 'description': \"This is a guide to PySpark code style presenting common situations and the associated best practices based on the most frequent recurring topics across the PySpark repos we've encountered.\"}\n",
      "{'name': 'kavgan/nlp-in-practice', 'url': 'https://github.com/kavgan/nlp-in-practice.git', 'stars': 1181, 'description': 'Starter code to solve real world text data problems. Includes: Gensim Word2Vec, phrase embeddings, Text Classification with Logistic Regression, word count with pyspark, simple text preprocessing, pre-trained embeddings and more.'}\n",
      "{'name': 'lensacom/sparkit-learn', 'url': 'https://github.com/lensacom/sparkit-learn.git', 'stars': 1154, 'description': 'PySpark + Scikit-learn = Sparkit-learn'}\n",
      "{'name': 'pyspark-ai/pyspark-ai', 'url': 'https://github.com/pyspark-ai/pyspark-ai.git', 'stars': 876, 'description': 'English SDK for Apache Spark'}\n",
      "{'name': 'lyhue1991/eat_pyspark_in_10_days', 'url': 'https://github.com/lyhue1991/eat_pyspark_in_10_days.git', 'stars': 821, 'description': 'pyspark🍒🥭  is delicious，just eat it!😋😋'}\n",
      "{'name': 'WeBankFinTech/Scriptis', 'url': 'https://github.com/WeBankFinTech/Scriptis.git', 'stars': 813, 'description': 'Scriptis is for interactive data analysis with script development(SQL, Pyspark, HiveQL), task submission(Spark, Hive), UDF, function, resource management and intelligent diagnosis.'}\n",
      "{'name': 'MrPowers/chispa', 'url': 'https://github.com/MrPowers/chispa.git', 'stars': 723, 'description': 'PySpark test helper methods with beautiful error messages'}\n",
      "{'name': 'mrpowers-io/quinn', 'url': 'https://github.com/mrpowers-io/quinn.git', 'stars': 674, 'description': 'pyspark methods to enhance developer productivity 📣 👯 🎉'}\n",
      "{'name': 'drabastomek/learningPySpark', 'url': 'https://github.com/drabastomek/learningPySpark.git', 'stars': 627, 'description': 'Code base for the Learning PySpark book (in preparation)'}\n",
      "{'name': 'kevinschaich/pyspark-cheatsheet', 'url': 'https://github.com/kevinschaich/pyspark-cheatsheet.git', 'stars': 624, 'description': '🐍 Quick reference guide to common patterns & functions in PySpark.'}\n",
      "{'name': 'krishnaik06/Pyspark-With-Python', 'url': 'https://github.com/krishnaik06/Pyspark-With-Python.git', 'stars': 514, 'description': None}\n",
      "{'name': 'cluster-apps-on-docker/spark-standalone-cluster-on-docker', 'url': 'https://github.com/cluster-apps-on-docker/spark-standalone-cluster-on-docker.git', 'stars': 495, 'description': 'Learn Apache Spark in Scala, Python (PySpark) and R (SparkR) by building your own cluster with a JupyterLab interface on Docker. :zap:'}\n",
      "{'name': 'cartershanklin/pyspark-cheatsheet', 'url': 'https://github.com/cartershanklin/pyspark-cheatsheet.git', 'stars': 479, 'description': 'PySpark Cheat Sheet - example code to help you learn PySpark and develop apps faster'}\n"
     ]
    }
   ],
   "source": [
    "query = \"pyspark in:name,description\"\n",
    "sort = \"stars\"\n",
    "order = \"desc\"\n",
    "limit = 20\n",
    "\n",
    "repos = g.search_repositories(query=query, sort=sort, order=order)\n",
    "\n",
    "popular_repos = []\n",
    "for i, repo in enumerate(repos):\n",
    "    if i >= limit:\n",
    "        break\n",
    "\n",
    "    popular_repos.append({\n",
    "        \"name\": repo.full_name,\n",
    "        \"url\": repo.clone_url,\n",
    "        \"stars\": repo.stargazers_count,\n",
    "        \"description\": repo.description\n",
    "    })\n",
    "\n",
    "for repo in popular_repos:\n",
    "    print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ff7305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 potential matches in AlexIoannides/pyspark-example-project\n",
      "\n",
      "Found 2 potential matches in uber/petastorm\n",
      "\n",
      "Found 17 potential matches in jadianes/spark-py-notebooks\n",
      "\n",
      "Found 0 potential matches in ptyadana/SQL-Data-Analysis-and-Visualization-Projects\n",
      "\n",
      "Found 192 potential matches in hi-primus/optimus\n",
      "\n",
      "Found 209 potential matches in spark-examples/pyspark-examples\n",
      "\n",
      "Found 0 potential matches in mahmoudparsian/pyspark-tutorial\n",
      "\n",
      "Found 0 potential matches in palantir/pyspark-style-guide\n",
      "\n",
      "Found 0 potential matches in kavgan/nlp-in-practice\n",
      "\n",
      "Found 0 potential matches in lensacom/sparkit-learn\n",
      "\n",
      "Found 1 potential matches in pyspark-ai/pyspark-ai\n",
      "\n",
      "Found 0 potential matches in lyhue1991/eat_pyspark_in_10_days\n",
      "\n",
      "Found 0 potential matches in WeBankFinTech/Scriptis\n",
      "\n",
      "Found 4 potential matches in MrPowers/chispa\n",
      "\n",
      "Found 18 potential matches in mrpowers-io/quinn\n",
      "\n",
      "Found 23 potential matches in drabastomek/learningPySpark\n",
      "\n",
      "Found 0 potential matches in kevinschaich/pyspark-cheatsheet\n",
      "\n",
      "Found 14 potential matches in krishnaik06/Pyspark-With-Python\n",
      "\n",
      "Found 0 potential matches in cluster-apps-on-docker/spark-standalone-cluster-on-docker\n",
      "\n",
      "Found 264 potential matches in cartershanklin/pyspark-cheatsheet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import json\n",
    "from nbconvert import PythonExporter\n",
    "import nbformat\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "# Create results jsonl\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "summary_path = os.path.join(\"results\", \"summary.jsonl\")\n",
    "\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as summary_file:\n",
    "    for repo in popular_repos:\n",
    "        repo_name = repo[\"name\"]\n",
    "        clone_url = repo[\"url\"]\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            # clone repo into temporary directory\n",
    "            repo_dir = os.path.join(tmpdir, \"repo\")\n",
    "            git.Repo.clone_from(clone_url, repo_dir, depth=1)\n",
    "\n",
    "            # find .ipynb files and convert into .py files\n",
    "            for root, _, files in os.walk(repo_dir):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".ipynb\"):\n",
    "                        ipynb_path = os.path.join(root, file)\n",
    "                        py_path = os.path.join(root, \"CONVERTED\"+file.replace(\".ipynb\", \".py\"))\n",
    "\n",
    "                        with open(ipynb_path, \"r\", encoding=\"utf-8\") as ipynbf:\n",
    "                            nb_node = nbformat.read(ipynbf, as_version=4)\n",
    "\n",
    "                            exporter = PythonExporter()\n",
    "                            try:\n",
    "                                python_code, _ = exporter.from_notebook_node(nb_node)\n",
    "                                with open(py_path, \"w\", encoding=\"utf-8\") as pyf:\n",
    "                                    pyf.write(python_code)\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "\n",
    "            # use semgrep to detect udf definitions\n",
    "            semgrep_result = subprocess.run(\n",
    "                [\"semgrep\", \"scan\", \"--config\", \"pyspark-rules.yml\", repo_dir, \"--json\"],\n",
    "                capture_output=True,\n",
    "                encoding=\"utf-8\", #quick fix: to avoid byte serialization issue on Windows laptops.\n",
    "                text=True,\n",
    "                check=False\n",
    "            )\n",
    "\n",
    "            # parse pyspark dataframe expressions and track udf usage\n",
    "            try:\n",
    "                data = json.loads(semgrep_result.stdout)\n",
    "                matches = data.get(\"results\", [])\n",
    "                print(f\"Found {len(matches)} potential matches in {repo_name}\\n\")\n",
    "\n",
    "                repo_results = {\n",
    "                    \"repo_name\" : repo_name,\n",
    "                    \"clone_url\" : clone_url,\n",
    "                }\n",
    "                \n",
    "                file_dic = {}\n",
    "                for match in matches:\n",
    "                    file_path = match[\"path\"]\n",
    "                    rel_path = os.path.relpath(file_path, repo_dir)\n",
    "                    if rel_path not in file_dic:\n",
    "                        file_dic[rel_path] = {\n",
    "                            \"udfs\": [],\n",
    "                            \"df_exprs\": []\n",
    "                        }\n",
    "\n",
    "                    start_offset = match[\"start\"][\"offset\"]\n",
    "                    end_offset = match[\"end\"][\"offset\"]\n",
    "                    with open(file_path, \"r\") as f:\n",
    "                        content = f.read()\n",
    "                        snippet = content[start_offset:end_offset]\n",
    "                    \n",
    "                        if match[\"check_id\"] == \"pyspark-udf-definition\":\n",
    "                            file_dic[rel_path][\"udfs\"].append({\n",
    "                                \"name\": match[\"extra\"][\"message\"],\n",
    "                                \"def\": snippet\n",
    "                            })\n",
    "\n",
    "                        elif match[\"check_id\"] == \"pyspark-df-expression\":\n",
    "                            file_dic[rel_path][\"df_exprs\"].append(snippet)\n",
    "\n",
    "                repo_results[\"files\"] = [{\"path\": k, **v} for k,v in file_dic.items()]\n",
    "\n",
    "                # write to results jsonl\n",
    "                summary_file.write(json.dumps(repo_results, ensure_ascii=False) + \"\\n\")\n",
    "                summary_file.flush()\n",
    "\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Semgrep output not valid JSON.\")\n",
    "                print(semgrep_result.stdout[:500])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
