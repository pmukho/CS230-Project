{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6938728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from github import Github, Auth\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"Specify GITHUB_TOKEN in .env file.\")\n",
    "\n",
    "g = Github(auth=Auth.Token(GITHUB_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0683cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'AlexIoannides/pyspark-example-project', 'url': 'https://github.com/AlexIoannides/pyspark-example-project.git', 'stars': 2013, 'description': 'Implementing best practices for PySpark ETL jobs and applications.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"pyspark in:name,description\"\n",
    "sort = \"stars\"\n",
    "order = \"desc\"\n",
    "limit = 1\n",
    "\n",
    "repos = g.search_repositories(query=query, sort=sort, order=order)\n",
    "\n",
    "popular_repos = []\n",
    "for i, repo in enumerate(repos):\n",
    "    if i >= limit:\n",
    "        break\n",
    "\n",
    "    popular_repos.append({\n",
    "        \"name\": repo.full_name,\n",
    "        \"url\": repo.clone_url,\n",
    "        \"stars\": repo.stargazers_count,\n",
    "        \"description\": repo.description\n",
    "    })\n",
    "\n",
    "for repo in popular_repos:\n",
    "    print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53ff7305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': '1.140.0', 'results': [{'check_id': 'tmp.tmpd67nisb5.pyspark-df-detection', 'path': '/tmp/tmpd67nisb5/repo/dependencies/spark.py', 'start': {'line': 77, 'col': 31, 'offset': 2951}, 'end': {'line': 77, 'col': 59, 'offset': 2979}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'tmp.tmpd67nisb5.pyspark-df-detection', 'path': '/tmp/tmpd67nisb5/repo/dependencies/spark.py', 'start': {'line': 80, 'col': 23, 'offset': 3076}, 'end': {'line': 80, 'col': 44, 'offset': 3097}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'tmp.tmpd67nisb5.pyspark-df-detection', 'path': '/tmp/tmpd67nisb5/repo/dependencies/spark.py', 'start': {'line': 98, 'col': 31, 'offset': 3730}, 'end': {'line': 98, 'col': 74, 'offset': 3773}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'tmp.tmpd67nisb5.pyspark-df-detection', 'path': '/tmp/tmpd67nisb5/repo/jobs/etl_job.py', 'start': {'line': 88, 'col': 22, 'offset': 2773}, 'end': {'line': 96, 'col': 79, 'offset': 3034}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'tmp.tmpd67nisb5.pyspark-df-detection', 'path': '/tmp/tmpd67nisb5/repo/jobs/etl_job.py', 'start': {'line': 107, 'col': 5, 'offset': 3195}, 'end': {'line': 110, 'col': 57, 'offset': 3285}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'tmp.tmpd67nisb5.pyspark-df-detection', 'path': '/tmp/tmpd67nisb5/repo/jobs/etl_job.py', 'start': {'line': 137, 'col': 5, 'offset': 4303}, 'end': {'line': 140, 'col': 62, 'offset': 4398}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}, {'check_id': 'tmp.tmpd67nisb5.pyspark-df-detection', 'path': '/tmp/tmpd67nisb5/repo/jobs/etl_job.py', 'start': {'line': 146, 'col': 5, 'offset': 4555}, 'end': {'line': 149, 'col': 69, 'offset': 4660}, 'extra': {'message': 'Detected PySpark DF operation.', 'metadata': {}, 'severity': 'INFO', 'fingerprint': 'requires login', 'lines': 'requires login', 'validation_state': 'NO_VALIDATOR', 'engine_kind': 'OSS'}}], 'errors': [], 'paths': {'scanned': ['/tmp/tmpd67nisb5/repo/dependencies/__init__.py', '/tmp/tmpd67nisb5/repo/dependencies/logging.py', '/tmp/tmpd67nisb5/repo/dependencies/spark.py', '/tmp/tmpd67nisb5/repo/jobs/etl_job.py']}, 'time': {'rules': [], 'rules_parse_time': 0.008169889450073242, 'profiling_times': {'config_time': 0.32985782623291016, 'core_time': 0.8268983364105225, 'ignores_time': 9.703636169433594e-05, 'total_time': 1.1574206352233887}, 'parsing_time': {'total_time': 0.0, 'per_file_time': {'mean': 0.0, 'std_dev': 0.0}, 'very_slow_stats': {'time_ratio': 0.0, 'count_ratio': 0.0}, 'very_slow_files': []}, 'scanning_time': {'total_time': 0.09901309013366699, 'per_file_time': {'mean': 0.024753272533416748, 'std_dev': 0.0001493760597774951}, 'very_slow_stats': {'time_ratio': 0.0, 'count_ratio': 0.0}, 'very_slow_files': []}, 'matching_time': {'total_time': 0.0, 'per_file_and_rule_time': {'mean': 0.0, 'std_dev': 0.0}, 'very_slow_stats': {'time_ratio': 0.0, 'count_ratio': 0.0}, 'very_slow_rules_on_files': []}, 'tainting_time': {'total_time': 0.0, 'per_def_and_rule_time': {'mean': 0.0, 'std_dev': 0.0}, 'very_slow_stats': {'time_ratio': 0.0, 'count_ratio': 0.0}, 'very_slow_rules_on_defs': []}, 'fixpoint_timeouts': [], 'prefiltering': {'project_level_time': 0.0, 'file_level_time': 0.0, 'rules_with_project_prefilters_ratio': 0.0, 'rules_with_file_prefilters_ratio': 1.0, 'rules_selected_ratio': 0.375, 'rules_matched_ratio': 0.375}, 'targets': [], 'total_bytes': 0, 'max_memory_bytes': 77630016}, 'engine_requested': 'OSS', 'skipped_rules': []}\n",
      "Found 7 potential UDF matches\n",
      "\n",
      "File:  /tmp/tmpd67nisb5/repo/dependencies/spark.py\n",
      "Detected PySpark DF operation.\n",
      "','.join(list(jar_packages))\n",
      "====================\n",
      "File:  /tmp/tmpd67nisb5/repo/dependencies/spark.py\n",
      "Detected PySpark DF operation.\n",
      "','.join(list(files))\n",
      "====================\n",
      "File:  /tmp/tmpd67nisb5/repo/dependencies/spark.py\n",
      "Detected PySpark DF operation.\n",
      "path.join(spark_files_dir, config_files[0])\n",
      "====================\n",
      "File:  /tmp/tmpd67nisb5/repo/jobs/etl_job.py\n",
      "Detected PySpark DF operation.\n",
      "(\n",
      "        df\n",
      "        .select(\n",
      "            col('id'),\n",
      "            concat_ws(\n",
      "                ' ',\n",
      "                col('first_name'),\n",
      "                col('second_name')).alias('name'),\n",
      "               (col('floor') * lit(steps_per_floor_)).alias('steps_to_desk')))\n",
      "====================\n",
      "File:  /tmp/tmpd67nisb5/repo/jobs/etl_job.py\n",
      "Detected PySpark DF operation.\n",
      "(df\n",
      "     .coalesce(1)\n",
      "     .write\n",
      "     .csv('loaded_data', mode='overwrite', header=True))\n",
      "====================\n",
      "File:  /tmp/tmpd67nisb5/repo/jobs/etl_job.py\n",
      "Detected PySpark DF operation.\n",
      "(df\n",
      "     .coalesce(1)\n",
      "     .write\n",
      "     .parquet('tests/test_data/employees', mode='overwrite'))\n",
      "====================\n",
      "File:  /tmp/tmpd67nisb5/repo/jobs/etl_job.py\n",
      "Detected PySpark DF operation.\n",
      "(df_tf\n",
      "     .coalesce(1)\n",
      "     .write\n",
      "     .parquet('tests/test_data/employees_report', mode='overwrite'))\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import json\n",
    "import tempfile\n",
    "import subprocess\n",
    "\n",
    "semgrep_rule = \"\"\"\n",
    "rules:\n",
    "  - id: pyspark-udf-detection\n",
    "    languages: [python]\n",
    "    message: \"Detected PySpark UDF definition or registration.\"\n",
    "    severity: INFO\n",
    "    patterns:\n",
    "      - pattern-either:\n",
    "          - pattern: |\n",
    "              @udf(...)\n",
    "              def $FUNC(...):\n",
    "                  ...\n",
    "          - pattern: |\n",
    "              def $FUNC(...):\n",
    "                  ...\n",
    "      - pattern-either:\n",
    "          - pattern: spark.udf.register($NAME, $FUNC)\n",
    "          - pattern: $SPARK.udf.register($NAME, $FUNC)\n",
    "          - pattern: F.udf(...)\n",
    "          - pattern: pyspark.sql.functions.udf(...)\n",
    "\n",
    "  - id: pyspark-df-detection\n",
    "    languages: [python]\n",
    "    message: \"Detected PySpark DF operation.\"\n",
    "    severity: INFO\n",
    "    patterns:\n",
    "      - pattern-either:\n",
    "          - pattern: $DF.select(...)\n",
    "          - pattern: $DF.filter(...)\n",
    "          - pattern: $DF.where(...)\n",
    "          - pattern: $DF.groupBy(...)\n",
    "          - pattern: $DF.agg(...)\n",
    "          - pattern: $DF.join(...)\n",
    "          - pattern: $DF.withColumn(...)\n",
    "          - pattern: $DF.drop(...)\n",
    "          - pattern: $DF.distinct(...)\n",
    "          - pattern: $DF.limit(...)\n",
    "          - pattern: $DF.union(...)\n",
    "          - pattern: $DF.orderBy(...)\n",
    "          - pattern: $DF.sort(...)\n",
    "          - pattern: $DF.write.$FUNC(...)\n",
    "          - pattern: $DF.write.format(...).save(...)\n",
    "          - pattern: $DF.write.mode(...).parquet(...)\n",
    "          - pattern: $DF.write.mode(...).csv(...)\n",
    "\"\"\"\n",
    "\n",
    "for repo in popular_repos:\n",
    "    clone_url = repo[\"url\"]\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        repo_dir = os.path.join(tmpdir, \"repo\")\n",
    "        git.Repo.clone_from(clone_url, repo_dir, depth=1)\n",
    "\n",
    "        rule_path = os.path.join(tmpdir, \"pyspark-udf.yaml\")\n",
    "        with open(rule_path, \"w\") as f:\n",
    "            f.write(semgrep_rule)\n",
    "\n",
    "        result = subprocess.run(\n",
    "            [\"semgrep\", \"scan\", \"--config\", rule_path, repo_dir, \"--json\"],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=False\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            data = json.loads(result.stdout)\n",
    "            print(data)\n",
    "            matches = data.get(\"results\", [])\n",
    "            print(f\"Found {len(matches)} potential UDF matches\\n\")\n",
    "\n",
    "            for match in matches:\n",
    "                file_path = match[\"path\"]\n",
    "                start_offset = match[\"start\"][\"offset\"]\n",
    "                end_offset = match[\"end\"][\"offset\"]\n",
    "\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                snippet = content[start_offset:end_offset]\n",
    "\n",
    "                print(\"File: \", file_path)\n",
    "                print(match[\"extra\"][\"message\"])\n",
    "                print(snippet)\n",
    "                print(\"=\"*20)\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Semgrep output not valid JSON.\")\n",
    "            print(result.stdout[:500])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
