{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6938728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from github import Github, Auth\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"Specify GITHUB_TOKEN in .env file.\")\n",
    "\n",
    "g = Github(auth=Auth.Token(GITHUB_TOKEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0683cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'AlexIoannides/pyspark-example-project', 'url': 'https://github.com/AlexIoannides/pyspark-example-project.git', 'stars': 2015, 'description': 'Implementing best practices for PySpark ETL jobs and applications.'}\n",
      "{'name': 'uber/petastorm', 'url': 'https://github.com/uber/petastorm.git', 'stars': 1865, 'description': 'Petastorm library enables single machine or distributed training and evaluation of deep learning models from datasets in Apache Parquet format. It supports ML frameworks such as Tensorflow, Pytorch, and PySpark and can be used from pure Python code.'}\n",
      "{'name': 'jadianes/spark-py-notebooks', 'url': 'https://github.com/jadianes/spark-py-notebooks.git', 'stars': 1667, 'description': 'Apache Spark & Python (pySpark) tutorials for Big Data Analysis and Machine Learning as IPython / Jupyter notebooks'}\n",
      "{'name': 'ptyadana/SQL-Data-Analysis-and-Visualization-Projects', 'url': 'https://github.com/ptyadana/SQL-Data-Analysis-and-Visualization-Projects.git', 'stars': 1583, 'description': 'SQL data analysis & visualization projects using MySQL, PostgreSQL, SQLite, Tableau, Apache Spark and pySpark.'}\n",
      "{'name': 'hi-primus/optimus', 'url': 'https://github.com/hi-primus/optimus.git', 'stars': 1524, 'description': ':truck: Agile Data Preparation Workflows made\\xa0easy with Pandas, Dask, cuDF, Dask-cuDF, Vaex and PySpark'}\n",
      "{'name': 'spark-examples/pyspark-examples', 'url': 'https://github.com/spark-examples/pyspark-examples.git', 'stars': 1316, 'description': 'Pyspark RDD, DataFrame and Dataset Examples in Python language'}\n",
      "{'name': 'mahmoudparsian/pyspark-tutorial', 'url': 'https://github.com/mahmoudparsian/pyspark-tutorial.git', 'stars': 1258, 'description': 'PySpark-Tutorial provides basic algorithms using PySpark'}\n",
      "{'name': 'palantir/pyspark-style-guide', 'url': 'https://github.com/palantir/pyspark-style-guide.git', 'stars': 1184, 'description': \"This is a guide to PySpark code style presenting common situations and the associated best practices based on the most frequent recurring topics across the PySpark repos we've encountered.\"}\n",
      "{'name': 'kavgan/nlp-in-practice', 'url': 'https://github.com/kavgan/nlp-in-practice.git', 'stars': 1181, 'description': 'Starter code to solve real world text data problems. Includes: Gensim Word2Vec, phrase embeddings, Text Classification with Logistic Regression, word count with pyspark, simple text preprocessing, pre-trained embeddings and more.'}\n",
      "{'name': 'lensacom/sparkit-learn', 'url': 'https://github.com/lensacom/sparkit-learn.git', 'stars': 1154, 'description': 'PySpark + Scikit-learn = Sparkit-learn'}\n",
      "{'name': 'pyspark-ai/pyspark-ai', 'url': 'https://github.com/pyspark-ai/pyspark-ai.git', 'stars': 876, 'description': 'English SDK for Apache Spark'}\n",
      "{'name': 'lyhue1991/eat_pyspark_in_10_days', 'url': 'https://github.com/lyhue1991/eat_pyspark_in_10_days.git', 'stars': 821, 'description': 'pysparkðŸ’ðŸ¥­  is deliciousï¼Œjust eat it!ðŸ˜‹ðŸ˜‹'}\n",
      "{'name': 'WeBankFinTech/Scriptis', 'url': 'https://github.com/WeBankFinTech/Scriptis.git', 'stars': 813, 'description': 'Scriptis is for interactive data analysis with script development(SQL, Pyspark, HiveQL), task submission(Spark, Hive), UDF, function, resource management and intelligent diagnosis.'}\n",
      "{'name': 'MrPowers/chispa', 'url': 'https://github.com/MrPowers/chispa.git', 'stars': 723, 'description': 'PySpark test helper methods with beautiful error messages'}\n",
      "{'name': 'mrpowers-io/quinn', 'url': 'https://github.com/mrpowers-io/quinn.git', 'stars': 674, 'description': 'pyspark methods to enhance developer productivity ðŸ“£ ðŸ‘¯ ðŸŽ‰'}\n",
      "{'name': 'drabastomek/learningPySpark', 'url': 'https://github.com/drabastomek/learningPySpark.git', 'stars': 627, 'description': 'Code base for the Learning PySpark book (in preparation)'}\n",
      "{'name': 'kevinschaich/pyspark-cheatsheet', 'url': 'https://github.com/kevinschaich/pyspark-cheatsheet.git', 'stars': 623, 'description': 'ðŸ Quick reference guide to common patterns & functions in PySpark.'}\n",
      "{'name': 'krishnaik06/Pyspark-With-Python', 'url': 'https://github.com/krishnaik06/Pyspark-With-Python.git', 'stars': 514, 'description': None}\n",
      "{'name': 'cluster-apps-on-docker/spark-standalone-cluster-on-docker', 'url': 'https://github.com/cluster-apps-on-docker/spark-standalone-cluster-on-docker.git', 'stars': 495, 'description': 'Learn Apache Spark in Scala, Python (PySpark) and R (SparkR) by building your own cluster with a JupyterLab interface on Docker. :zap:'}\n",
      "{'name': 'cartershanklin/pyspark-cheatsheet', 'url': 'https://github.com/cartershanklin/pyspark-cheatsheet.git', 'stars': 479, 'description': 'PySpark Cheat Sheet - example code to help you learn PySpark and develop apps faster'}\n",
      "{'name': 'ericxiao251/spark-syntax', 'url': 'https://github.com/ericxiao251/spark-syntax.git', 'stars': 462, 'description': 'This is a repo documenting the best practices in PySpark.'}\n",
      "{'name': 'LucaCanali/Miscellaneous', 'url': 'https://github.com/LucaCanali/Miscellaneous.git', 'stars': 455, 'description': \"Includes notes on using Apache Spark, with drill down on  Spark for Physics, how to run TPCDS on PySpark, how to create histograms with Spark. Also tools for stress testing, measuring CPUs' performance, and I/O latency heat maps. Jupyter notebooks examples for using various DB systems.\"}\n",
      "{'name': 'eakmanrq/sqlframe', 'url': 'https://github.com/eakmanrq/sqlframe.git', 'stars': 443, 'description': 'Turning PySpark Into a Universal DataFrame API'}\n",
      "{'name': 'commoncrawl/cc-pyspark', 'url': 'https://github.com/commoncrawl/cc-pyspark.git', 'stars': 440, 'description': 'Process Common Crawl data with Python and Spark'}\n",
      "{'name': 'UrbanInstitute/pyspark-tutorials', 'url': 'https://github.com/UrbanInstitute/pyspark-tutorials.git', 'stars': 422, 'description': 'Code snippets and tutorials for working with social science data in PySpark'}\n",
      "{'name': 'CamDavidsonPilon/tdigest', 'url': 'https://github.com/CamDavidsonPilon/tdigest.git', 'stars': 402, 'description': 't-Digest data structure in Python. Useful for percentiles and quantiles, including distributed enviroments like PySpark'}\n",
      "{'name': 'ekampf/PySpark-Boilerplate', 'url': 'https://github.com/ekampf/PySpark-Boilerplate.git', 'stars': 394, 'description': 'A boilerplate for writing PySpark Jobs'}\n",
      "{'name': 'tirthajyoti/Spark-with-Python', 'url': 'https://github.com/tirthajyoti/Spark-with-Python.git', 'stars': 354, 'description': 'Fundamentals of Spark with Python (using PySpark), code examples'}\n",
      "{'name': 'PacktPublishing/Learning-PySpark', 'url': 'https://github.com/PacktPublishing/Learning-PySpark.git', 'stars': 338, 'description': 'Code repository for Learning PySpark by Packt'}\n",
      "{'name': 'databrickslabs/dqx', 'url': 'https://github.com/databrickslabs/dqx.git', 'stars': 328, 'description': 'Databricks framework to validate Data Quality of pySpark DataFrames'}\n",
      "{'name': 'MrPowers/mack', 'url': 'https://github.com/MrPowers/mack.git', 'stars': 323, 'description': 'Delta Lake helper methods in PySpark'}\n",
      "{'name': 'susanli2016/PySpark-and-MLlib', 'url': 'https://github.com/susanli2016/PySpark-and-MLlib.git', 'stars': 300, 'description': 'Getting start with PySpark and MLlib'}\n",
      "{'name': 'MingChen0919/learning-apache-spark', 'url': 'https://github.com/MingChen0919/learning-apache-spark.git', 'stars': 297, 'description': 'Notes on Apache Spark (pyspark)'}\n",
      "{'name': 'Ibotta/sk-dist', 'url': 'https://github.com/Ibotta/sk-dist.git', 'stars': 286, 'description': 'Distributed scikit-learn meta-estimators in PySpark'}\n",
      "{'name': 'XD-DENG/Spark-practice', 'url': 'https://github.com/XD-DENG/Spark-practice.git', 'stars': 274, 'description': 'Apache Spark (PySpark) Practice on Real Data'}\n",
      "{'name': 'svenkreiss/pysparkling', 'url': 'https://github.com/svenkreiss/pysparkling.git', 'stars': 271, 'description': \"A pure Python implementation of Apache Spark's RDD and DStream interfaces.\"}\n",
      "{'name': 'PiercingDan/spark-Jupyter-AWS', 'url': 'https://github.com/PiercingDan/spark-Jupyter-AWS.git', 'stars': 261, 'description': 'A guide on how to set up Jupyter with Pyspark painlessly on AWS EC2 clusters, with S3 I/O support'}\n",
      "{'name': 'ThreatHuntingProject/hunter', 'url': 'https://github.com/ThreatHuntingProject/hunter.git', 'stars': 249, 'description': 'A threat hunting / data analysis environment based on Python, Pandas, PySpark and Jupyter Notebook.'}\n",
      "{'name': 'G-Research/spark-extension', 'url': 'https://github.com/G-Research/spark-extension.git', 'stars': 231, 'description': 'A library that provides useful extensions to Apache Spark and PySpark.'}\n",
      "{'name': 'davidzajac1/zillacode', 'url': 'https://github.com/davidzajac1/zillacode.git', 'stars': 222, 'description': 'Open Source LeetCode for PySpark, Spark, Pandas and DBT/Snowflake'}\n",
      "{'name': 'jonesberg/DataAnalysisWithPythonAndPySpark', 'url': 'https://github.com/jonesberg/DataAnalysisWithPythonAndPySpark.git', 'stars': 209, 'description': 'Code repository for the \"PySpark in Action\" book'}\n",
      "{'name': 'locationtech-labs/geopyspark', 'url': 'https://github.com/locationtech-labs/geopyspark.git', 'stars': 180, 'description': 'GeoTrellis for PySpark'}\n",
      "{'name': 'jkthompson/pyspark-pictures', 'url': 'https://github.com/jkthompson/pyspark-pictures.git', 'stars': 170, 'description': 'Learn the pyspark API through pictures and simple examples'}\n",
      "{'name': 'PacktPublishing/Mastering-Big-Data-Analytics-with-PySpark', 'url': 'https://github.com/PacktPublishing/Mastering-Big-Data-Analytics-with-PySpark.git', 'stars': 162, 'description': 'Mastering Big Data Analytics with PySpark, Published by Packt'}\n",
      "{'name': 'mahmoudparsian/big-data-mapreduce-course', 'url': 'https://github.com/mahmoudparsian/big-data-mapreduce-course.git', 'stars': 161, 'description': 'Big Data Modeling, MapReduce, Spark, PySpark @ Santa Clara University'}\n",
      "{'name': 'awantik/pyspark-learning', 'url': 'https://github.com/awantik/pyspark-learning.git', 'stars': 157, 'description': 'Updated repository'}\n",
      "{'name': 'raveendratal/PysparkRaveendra', 'url': 'https://github.com/raveendratal/PysparkRaveendra.git', 'stars': 148, 'description': 'Git Repository'}\n",
      "{'name': 'coder2j/pyspark-tutorial', 'url': 'https://github.com/coder2j/pyspark-tutorial.git', 'stars': 135, 'description': 'PySpark Tutorial for Beginners - Practical Examples in Jupyter Notebook with Spark version 3.4.1. The tutorial covers various topics like Spark Introduction, Spark Installation, Spark RDD Transformations and Actions, Spark DataFrame, Spark SQL, and more. It is completely free on YouTube and is beginner-friendly without any prerequisites.'}\n",
      "{'name': 'AISCIENCES/course-master-big-data-with-pyspark-and-aws', 'url': 'https://github.com/AISCIENCES/course-master-big-data-with-pyspark-and-aws.git', 'stars': 131, 'description': 'Master Big Data With PySpark and AWS'}\n",
      "{'name': 'Azure/PySpark-Predictive-Maintenance', 'url': 'https://github.com/Azure/PySpark-Predictive-Maintenance.git', 'stars': 129, 'description': 'Predictive Maintenance using Pyspark'}\n",
      "{'name': 'thinline72/nsl-kdd', 'url': 'https://github.com/thinline72/nsl-kdd.git', 'stars': 126, 'description': 'PySpark solution to the NSL-KDD dataset: https://www.unb.ca/cic/datasets/nsl.html'}\n",
      "{'name': 'evancasey/spark-knn-recommender', 'url': 'https://github.com/evancasey/spark-knn-recommender.git', 'stars': 124, 'description': 'Item and User-based KNN recommendation algorithms using PySpark'}\n",
      "{'name': 'zero323/pyspark-stubs', 'url': 'https://github.com/zero323/pyspark-stubs.git', 'stars': 118, 'description': 'Apache (Py)Spark type annotations  (stub files).'}\n",
      "{'name': 'edyoda/pyspark-tutorial', 'url': 'https://github.com/edyoda/pyspark-tutorial.git', 'stars': 116, 'description': 'PySpark Code for Hands-on Learners '}\n",
      "{'name': 'Apress/machine-learning-with-pyspark', 'url': 'https://github.com/Apress/machine-learning-with-pyspark.git', 'stars': 116, 'description': \"Source Code for 'Machine Learning with PySpark' by Pramod Singh\"}\n",
      "{'name': 'vivek-bombatkar/Spark-with-Python---My-learning-notes-', 'url': 'https://github.com/vivek-bombatkar/Spark-with-Python---My-learning-notes-.git', 'stars': 116, 'description': 'ETL pipeline using pyspark (Spark - Python)'}\n",
      "{'name': 'DataBora/elusion', 'url': 'https://github.com/DataBora/elusion.git', 'stars': 113, 'description': 'DataFrame / Data Engineering Library with familiar syntax like ones we love: PySpark and SQL, focused on user experience, spead and accuracy. '}\n",
      "{'name': 'andfanilo/pyspark-tutorial', 'url': 'https://github.com/andfanilo/pyspark-tutorial.git', 'stars': 110, 'description': 'Jupyter notebooks for pyspark tutorials given at University'}\n",
      "{'name': 'mitchelllisle/sparkdantic', 'url': 'https://github.com/mitchelllisle/sparkdantic.git', 'stars': 109, 'description': 'âœ¨ A Pydantic to PySpark schema library'}\n",
      "{'name': 'abulbasar/pyspark-examples', 'url': 'https://github.com/abulbasar/pyspark-examples.git', 'stars': 108, 'description': 'Code examples on Apache Spark using python'}\n",
      "{'name': 'hyunjoonbok/PySpark', 'url': 'https://github.com/hyunjoonbok/PySpark.git', 'stars': 104, 'description': 'PySpark functions and utilities with examples. Assists ETL process of data modeling'}\n",
      "{'name': 'Bergvca/pyspark_dist_explore', 'url': 'https://github.com/Bergvca/pyspark_dist_explore.git', 'stars': 102, 'description': 'Data Exploration in PySpark made easy - Pyspark_dist_explore provides methods to get fast insights in your Spark DataFrames.'}\n",
      "{'name': 'martandsingh/ApacheSpark', 'url': 'https://github.com/martandsingh/ApacheSpark.git', 'stars': 102, 'description': 'This repository will help you to learn about databricks concept with the help of examples. It will include all the important topics which we need in our real life experience as a data engineer. We will be using pyspark & sparksql for the development. At the end of the course we also cover few case studies.'}\n",
      "{'name': 'jadianes/kdd-cup-99-spark', 'url': 'https://github.com/jadianes/kdd-cup-99-spark.git', 'stars': 100, 'description': 'PySpark solution to the KDDCup99'}\n",
      "{'name': 'subhamkharwal/pyspark-zero-to-hero', 'url': 'https://github.com/subhamkharwal/pyspark-zero-to-hero.git', 'stars': 99, 'description': 'Learn PySpark from Basics to Advanced. Checkout the YouTube Series : [PySpark - Zero to Hero]'}\n",
      "{'name': 'jpmml/pyspark2pmml', 'url': 'https://github.com/jpmml/pyspark2pmml.git', 'stars': 98, 'description': 'Python library for converting Apache Spark ML pipelines to PMML'}\n",
      "{'name': 'SuperJohn/spark-and-python-for-big-data-with-pyspark', 'url': 'https://github.com/SuperJohn/spark-and-python-for-big-data-with-pyspark.git', 'stars': 98, 'description': 'Course on Udemy by Jose Portilla'}\n",
      "{'name': 'HanXiaoyang/pyspark-recommendation-demo', 'url': 'https://github.com/HanXiaoyang/pyspark-recommendation-demo.git', 'stars': 95, 'description': 'movie recommendation demo using collaborative filtering and lfm(spark mllib ALS)'}\n",
      "{'name': 'PacktPublishing/PySpark-Cookbook', 'url': 'https://github.com/PacktPublishing/PySpark-Cookbook.git', 'stars': 93, 'description': 'PySpark Cookbook, published by Packt'}\n",
      "{'name': 'mrn-aglic/pyspark-playground', 'url': 'https://github.com/mrn-aglic/pyspark-playground.git', 'stars': 91, 'description': None}\n",
      "{'name': 'johnny-chivers/pyspark-glue-tutorial', 'url': 'https://github.com/johnny-chivers/pyspark-glue-tutorial.git', 'stars': 90, 'description': None}\n",
      "{'name': 'seahboonsiew/pyspark-csv', 'url': 'https://github.com/seahboonsiew/pyspark-csv.git', 'stars': 90, 'description': \"An external PySpark module that works like R's read.csv or Panda's read_csv, with automatic type inference and null value handling. Parses csv data into SchemaRDD. No installation required, simply include pyspark_csv.py via SparkContext.\"}\n",
      "{'name': 'kaiko-ai/typedspark', 'url': 'https://github.com/kaiko-ai/typedspark.git', 'stars': 88, 'description': 'Column-wise type annotations for pyspark DataFrames'}\n",
      "{'name': 'mahmoudparsian/pyspark-algorithms', 'url': 'https://github.com/mahmoudparsian/pyspark-algorithms.git', 'stars': 87, 'description': 'PySpark Algorithms Book: https://www.amazon.com/dp/B07X4B2218/ref=sr_1_2'}\n",
      "{'name': 'malexer/pytest-spark', 'url': 'https://github.com/malexer/pytest-spark.git', 'stars': 87, 'description': 'pytest plugin to run the tests with support of pyspark'}\n",
      "{'name': 'kawadia/pyspark.test', 'url': 'https://github.com/kawadia/pyspark.test.git', 'stars': 84, 'description': 'Example unit tests for Apache Spark Python scripts using the py.test framework'}\n",
      "{'name': 'TargetHolding/pyspark-cassandra', 'url': 'https://github.com/TargetHolding/pyspark-cassandra.git', 'stars': 79, 'description': 'PySpark Cassandra brings back the fun in working with Cassandra data in PySpark.'}\n",
      "{'name': 'sabman/PySparkGeoAnalysis', 'url': 'https://github.com/sabman/PySparkGeoAnalysis.git', 'stars': 79, 'description': ':globe_with_meridians: Interactive Workshop on GeoAnalysis using PySpark'}\n",
      "{'name': 'ogrisel/spylearn', 'url': 'https://github.com/ogrisel/spylearn.git', 'stars': 79, 'description': 'Repo for experiments on pyspark and sklearn'}\n",
      "{'name': 'mrugankray/Big-Data-Cluster', 'url': 'https://github.com/mrugankray/Big-Data-Cluster.git', 'stars': 74, 'description': 'The goal of this project is to build a docker cluster that gives access to Hadoop, HDFS, Hive, PySpark, Sqoop, Airflow, Kafka, Flume, Postgres, Cassandra, Hue,  Zeppelin, Kadmin, Kafka Control Center and pgAdmin. This cluster is solely intended for usage in a development environment. Do not use it to run any production workloads.'}\n",
      "{'name': 'Learn-Apache-Spark/SparkML', 'url': 'https://github.com/Learn-Apache-Spark/SparkML.git', 'stars': 71, 'description': 'Spark ML with pyspark'}\n",
      "{'name': 'allisonwang-db/pyspark-data-sources', 'url': 'https://github.com/allisonwang-db/pyspark-data-sources.git', 'stars': 71, 'description': 'Custom PySpark Data Sources'}\n",
      "{'name': 'anguenot/pyspark-cassandra', 'url': 'https://github.com/anguenot/pyspark-cassandra.git', 'stars': 69, 'description': 'pyspark-cassandra is a Python port of the awesome @datastax Spark Cassandra connector. Compatible w/ Spark 2.0, 2.1, 2.2, 2.3 and 2.4'}\n",
      "{'name': 'zaratsian/Spark', 'url': 'https://github.com/zaratsian/Spark.git', 'stars': 69, 'description': 'Apache Spark (Scala, PySpark, SparkR) Code, Tricks, and References'}\n",
      "{'name': 'amir-rahnama/pyspark-twitter-stream-mining', 'url': 'https://github.com/amir-rahnama/pyspark-twitter-stream-mining.git', 'stars': 68, 'description': 'Real-time Machine Learning with Apache Spark on Twitter Public Stream'}\n",
      "{'name': 'iobruno/data-engineering-examples', 'url': 'https://github.com/iobruno/data-engineering-examples.git', 'stars': 68, 'description': 'Data Engineering examples for Airflow, Prefect; dbt for BigQuery, Redshift, ClickHouse, Postgres, DuckDB; PySpark for Batch processing; Kafka for Stream processing'}\n",
      "{'name': 'sbl-sdsc/mmtf-pyspark', 'url': 'https://github.com/sbl-sdsc/mmtf-pyspark.git', 'stars': 67, 'description': 'Methods for the parallel and distributed analysis and mining of the Protein Data Bank using MMTF and Apache Spark.'}\n",
      "{'name': 'sodadata/soda-spark', 'url': 'https://github.com/sodadata/soda-spark.git', 'stars': 64, 'description': 'Soda Spark is a PySpark library that helps you with testing your data in Spark Dataframes'}\n",
      "{'name': 'dogukannulu/streaming_data_processing', 'url': 'https://github.com/dogukannulu/streaming_data_processing.git', 'stars': 63, 'description': 'Create a streaming data, transfer it to Kafka, modify it with PySpark, take it to ElasticSearch and MinIO'}\n",
      "{'name': 'asdspal/dimRed', 'url': 'https://github.com/asdspal/dimRed.git', 'stars': 62, 'description': 'python, scala, and pyspark code for few dimensional reduction algorithms'}\n",
      "{'name': 'tubular/sparkly', 'url': 'https://github.com/tubular/sparkly.git', 'stars': 62, 'description': 'Helpers & syntactic sugar for PySpark. '}\n",
      "{'name': 'sparkdq-community/sparkdq', 'url': 'https://github.com/sparkdq-community/sparkdq.git', 'stars': 61, 'description': 'A declarative PySpark framework for row- and aggregate-level data quality validation.'}\n",
      "{'name': 'drabastomek/PySparkCookbook', 'url': 'https://github.com/drabastomek/PySparkCookbook.git', 'stars': 60, 'description': 'A repository for a PySpark Cookbook by Tomasz Drabas and Denny Lee'}\n",
      "{'name': 'RubensZimbres/Repo-2021', 'url': 'https://github.com/RubensZimbres/Repo-2021.git', 'stars': 59, 'description': 'Transformers, Graph Neural Networks, PySpark, Neural Cellular Automata, FB Prophet, Google Cloud, NLP codes, Ethical Hacking and C Language'}\n",
      "{'name': 'FavioVazquez/deep-learning-pyspark', 'url': 'https://github.com/FavioVazquez/deep-learning-pyspark.git', 'stars': 58, 'description': 'Deep Learning with Apache Spark and Deep Cognition'}\n",
      "{'name': 'bensadeghi/pyspark-churn-prediction', 'url': 'https://github.com/bensadeghi/pyspark-churn-prediction.git', 'stars': 58, 'description': 'Churn Prediction with PySpark using MLlib and ML Packages'}\n",
      "{'name': 'KristianHolsheimer/pyspark-setup-guide', 'url': 'https://github.com/KristianHolsheimer/pyspark-setup-guide.git', 'stars': 56, 'description': 'A guide for setting up Spark + PySpark under Ubuntu linux'}\n",
      "{'name': 'subhamkharwal/ease-with-apache-spark', 'url': 'https://github.com/subhamkharwal/ease-with-apache-spark.git', 'stars': 56, 'description': 'Series follows learning from Apache Spark (PySpark) with quick tips and workaround for daily problems in hand'}\n",
      "{'name': 'sundarramamurthy/pyspark', 'url': 'https://github.com/sundarramamurthy/pyspark.git', 'stars': 55, 'description': 'A quick reference guide to the most commonly used patterns and functions in PySpark SQL'}\n",
      "{'name': 'jplane/pyspark-devcontainer', 'url': 'https://github.com/jplane/pyspark-devcontainer.git', 'stars': 55, 'description': 'A simple VS Code devcontainer setup for local PySpark development'}\n"
     ]
    }
   ],
   "source": [
    "query = \"pyspark in:name,description\"\n",
    "sort = \"stars\"\n",
    "order = \"desc\"\n",
    "limit = 100\n",
    "\n",
    "repos = g.search_repositories(query=query, sort=sort, order=order)\n",
    "\n",
    "popular_repos = []\n",
    "for i, repo in enumerate(repos):\n",
    "    if i >= limit:\n",
    "        break\n",
    "\n",
    "    popular_repos.append({\n",
    "        \"name\": repo.full_name,\n",
    "        \"url\": repo.clone_url,\n",
    "        \"stars\": repo.stargazers_count,\n",
    "        \"description\": repo.description\n",
    "    })\n",
    "\n",
    "for repo in popular_repos:\n",
    "    print(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53ff7305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 potential matches in AlexIoannides/pyspark-example-project\n",
      "\n",
      "Found 325 potential matches in uber/petastorm\n",
      "\n",
      "Found 32 potential matches in jadianes/spark-py-notebooks\n",
      "\n",
      "Found 0 potential matches in ptyadana/SQL-Data-Analysis-and-Visualization-Projects\n",
      "\n",
      "Found 1965 potential matches in hi-primus/optimus\n",
      "\n",
      "Found 209 potential matches in spark-examples/pyspark-examples\n",
      "\n",
      "Found 0 potential matches in mahmoudparsian/pyspark-tutorial\n",
      "\n",
      "Found 7 potential matches in palantir/pyspark-style-guide\n",
      "\n",
      "Found 24 potential matches in kavgan/nlp-in-practice\n",
      "\n",
      "Found 159 potential matches in lensacom/sparkit-learn\n",
      "\n",
      "Found 67 potential matches in pyspark-ai/pyspark-ai\n",
      "\n",
      "Found 0 potential matches in lyhue1991/eat_pyspark_in_10_days\n",
      "\n",
      "Found 0 potential matches in WeBankFinTech/Scriptis\n",
      "\n",
      "Found 49 potential matches in MrPowers/chispa\n",
      "\n",
      "Found 135 potential matches in mrpowers-io/quinn\n",
      "\n",
      "Found 32 potential matches in drabastomek/learningPySpark\n",
      "\n",
      "Found 0 potential matches in kevinschaich/pyspark-cheatsheet\n",
      "\n",
      "Found 14 potential matches in krishnaik06/Pyspark-With-Python\n",
      "\n",
      "Found 0 potential matches in cluster-apps-on-docker/spark-standalone-cluster-on-docker\n",
      "\n",
      "Found 305 potential matches in cartershanklin/pyspark-cheatsheet\n",
      "\n",
      "Found 32 potential matches in ericxiao251/spark-syntax\n",
      "\n",
      "Found 313 potential matches in LucaCanali/Miscellaneous\n",
      "\n",
      "Found 1372 potential matches in eakmanrq/sqlframe\n",
      "\n",
      "Found 41 potential matches in commoncrawl/cc-pyspark\n",
      "\n",
      "Found 31 potential matches in UrbanInstitute/pyspark-tutorials\n",
      "\n",
      "Found 5 potential matches in CamDavidsonPilon/tdigest\n",
      "\n",
      "Found 3 potential matches in ekampf/PySpark-Boilerplate\n",
      "\n",
      "Found 143 potential matches in tirthajyoti/Spark-with-Python\n",
      "\n",
      "Found 32 potential matches in PacktPublishing/Learning-PySpark\n",
      "\n",
      "Found 604 potential matches in databrickslabs/dqx\n",
      "\n",
      "Found 21 potential matches in MrPowers/mack\n",
      "\n",
      "Found 96 potential matches in susanli2016/PySpark-and-MLlib\n",
      "\n",
      "Found 109 potential matches in MingChen0919/learning-apache-spark\n",
      "\n",
      "Found 196 potential matches in Ibotta/sk-dist\n",
      "\n",
      "Found 1 potential matches in XD-DENG/Spark-practice\n",
      "\n",
      "Found 618 potential matches in svenkreiss/pysparkling\n",
      "\n",
      "Found 0 potential matches in PiercingDan/spark-Jupyter-AWS\n",
      "\n",
      "Found 1 potential matches in ThreatHuntingProject/hunter\n",
      "\n",
      "Found 84 potential matches in G-Research/spark-extension\n",
      "\n",
      "Found 31 potential matches in davidzajac1/zillacode\n",
      "\n",
      "Found 4 potential matches in jonesberg/DataAnalysisWithPythonAndPySpark\n",
      "\n",
      "Found 258 potential matches in locationtech-labs/geopyspark\n",
      "\n",
      "Found 0 potential matches in jkthompson/pyspark-pictures\n",
      "\n",
      "Found 61 potential matches in PacktPublishing/Mastering-Big-Data-Analytics-with-PySpark\n",
      "\n",
      "Found 22 potential matches in mahmoudparsian/big-data-mapreduce-course\n",
      "\n",
      "Found 102 potential matches in awantik/pyspark-learning\n",
      "\n",
      "Found 20 potential matches in raveendratal/PysparkRaveendra\n",
      "\n",
      "Found 14 potential matches in coder2j/pyspark-tutorial\n",
      "\n",
      "Found 99 potential matches in AISCIENCES/course-master-big-data-with-pyspark-and-aws\n",
      "\n",
      "Found 46 potential matches in Azure/PySpark-Predictive-Maintenance\n",
      "\n",
      "Found 175 potential matches in thinline72/nsl-kdd\n",
      "\n",
      "Found 15 potential matches in evancasey/spark-knn-recommender\n",
      "\n",
      "Found 2 potential matches in zero323/pyspark-stubs\n",
      "\n",
      "Found 132 potential matches in edyoda/pyspark-tutorial\n",
      "\n",
      "Found 176 potential matches in Apress/machine-learning-with-pyspark\n",
      "\n",
      "Found 66 potential matches in vivek-bombatkar/Spark-with-Python---My-learning-notes-\n",
      "\n",
      "Found 0 potential matches in DataBora/elusion\n",
      "\n",
      "Found 1 potential matches in andfanilo/pyspark-tutorial\n",
      "\n",
      "Found 15 potential matches in mitchelllisle/sparkdantic\n",
      "\n",
      "Found 47 potential matches in abulbasar/pyspark-examples\n",
      "\n",
      "Found 130 potential matches in hyunjoonbok/PySpark\n",
      "\n",
      "Found 30 potential matches in Bergvca/pyspark_dist_explore\n",
      "\n",
      "Found 167 potential matches in martandsingh/ApacheSpark\n",
      "\n",
      "Found 3 potential matches in jadianes/kdd-cup-99-spark\n",
      "\n",
      "Found 21 potential matches in subhamkharwal/pyspark-zero-to-hero\n",
      "\n",
      "Found 4 potential matches in jpmml/pyspark2pmml\n",
      "\n",
      "Found 169 potential matches in SuperJohn/spark-and-python-for-big-data-with-pyspark\n",
      "\n",
      "Found 12 potential matches in HanXiaoyang/pyspark-recommendation-demo\n",
      "\n",
      "Found 13 potential matches in PacktPublishing/PySpark-Cookbook\n",
      "\n",
      "Found 27 potential matches in mrn-aglic/pyspark-playground\n",
      "\n",
      "Found 0 potential matches in johnny-chivers/pyspark-glue-tutorial\n",
      "\n",
      "Found 7 potential matches in seahboonsiew/pyspark-csv\n",
      "\n",
      "Found 90 potential matches in kaiko-ai/typedspark\n",
      "\n",
      "Found 110 potential matches in mahmoudparsian/pyspark-algorithms\n",
      "\n",
      "Found 9 potential matches in malexer/pytest-spark\n",
      "\n",
      "Found 12 potential matches in kawadia/pyspark.test\n",
      "\n",
      "Found 59 potential matches in TargetHolding/pyspark-cassandra\n",
      "\n",
      "Found 22 potential matches in sabman/PySparkGeoAnalysis\n",
      "\n",
      "Found 20 potential matches in ogrisel/spylearn\n",
      "\n",
      "Found 0 potential matches in mrugankray/Big-Data-Cluster\n",
      "\n",
      "Found 70 potential matches in Learn-Apache-Spark/SparkML\n",
      "\n",
      "Found 74 potential matches in allisonwang-db/pyspark-data-sources\n",
      "\n",
      "Found 92 potential matches in anguenot/pyspark-cassandra\n",
      "\n",
      "Found 96 potential matches in zaratsian/Spark\n",
      "\n",
      "Found 7 potential matches in amir-rahnama/pyspark-twitter-stream-mining\n",
      "\n",
      "Found 74 potential matches in iobruno/data-engineering-examples\n",
      "\n",
      "Found 390 potential matches in sbl-sdsc/mmtf-pyspark\n",
      "\n",
      "Found 22 potential matches in sodadata/soda-spark\n",
      "\n",
      "Found 37 potential matches in dogukannulu/streaming_data_processing\n",
      "\n",
      "Found 194 potential matches in asdspal/dimRed\n",
      "\n",
      "Found 85 potential matches in tubular/sparkly\n",
      "\n",
      "Found 126 potential matches in sparkdq-community/sparkdq\n",
      "\n",
      "Found 13 potential matches in drabastomek/PySparkCookbook\n",
      "\n",
      "Found 1138 potential matches in RubensZimbres/Repo-2021\n",
      "\n",
      "Found 14 potential matches in FavioVazquez/deep-learning-pyspark\n",
      "\n",
      "Found 3 potential matches in bensadeghi/pyspark-churn-prediction\n",
      "\n",
      "Found 0 potential matches in KristianHolsheimer/pyspark-setup-guide\n",
      "\n",
      "Found 109 potential matches in subhamkharwal/ease-with-apache-spark\n",
      "\n",
      "Found 0 potential matches in sundarramamurthy/pyspark\n",
      "\n",
      "Found 0 potential matches in jplane/pyspark-devcontainer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import json\n",
    "from nbconvert import PythonExporter\n",
    "import nbformat\n",
    "import tempfile\n",
    "import subprocess\n",
    "import ast\n",
    "\n",
    "# Create results jsonl\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "summary_path = os.path.join(\"results\", \"summary.jsonl\")\n",
    "\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as summary_file:\n",
    "    for repo in popular_repos:\n",
    "        repo_name = repo[\"name\"]\n",
    "        clone_url = repo[\"url\"]\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            # clone repo into temporary directory\n",
    "            repo_dir = os.path.join(tmpdir, \"repo\")\n",
    "            git.Repo.clone_from(clone_url, repo_dir, depth=1)\n",
    "\n",
    "            # find .ipynb files and convert into .py files\n",
    "            for root, _, files in os.walk(repo_dir):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".ipynb\"):\n",
    "                        ipynb_path = os.path.join(root, file)\n",
    "                        py_path = os.path.join(root, \"CONVERTED\"+file.replace(\".ipynb\", \".py\"))\n",
    "\n",
    "                        with open(ipynb_path, \"r\", encoding=\"utf-8\") as ipynbf:\n",
    "                            nb_node = nbformat.read(ipynbf, as_version=4)\n",
    "\n",
    "                            exporter = PythonExporter()\n",
    "                            try:\n",
    "                                python_code, _ = exporter.from_notebook_node(nb_node)\n",
    "                                with open(py_path, \"w\", encoding=\"utf-8\") as pyf:\n",
    "                                    pyf.write(python_code)\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "\n",
    "            # use semgrep to detect udf definitions\n",
    "            semgrep_result = subprocess.run(\n",
    "                [\"semgrep\", \"scan\", \"--config\", \"pyspark-rules.yml\", repo_dir, \"--json\"],\n",
    "                capture_output=True,\n",
    "                encoding=\"utf-8\", #quick fix: to avoid byte serialization issue on Windows laptops.\n",
    "                text=True,\n",
    "                check=False\n",
    "            )\n",
    "\n",
    "            # parse pyspark dataframe expressions and track udf usage\n",
    "            try:\n",
    "                data = json.loads(semgrep_result.stdout)\n",
    "                matches = data.get(\"results\", [])\n",
    "                print(f\"Found {len(matches)} potential matches in {repo_name}\\n\")\n",
    "\n",
    "                repo_results = {\n",
    "                    \"repo_name\" : repo_name,\n",
    "                    \"clone_url\" : clone_url,\n",
    "                }\n",
    "                \n",
    "                file_dic = {}\n",
    "                for match in matches:\n",
    "                    if match[\"check_id\"] == \"library-usage\":\n",
    "                        continue # process dataframe expressions & udf definitions first\n",
    "\n",
    "                    file_path = match[\"path\"]\n",
    "                    rel_path = os.path.relpath(file_path, repo_dir)\n",
    "                    if rel_path not in file_dic:\n",
    "                        file_dic[rel_path] = {\n",
    "                            \"udfs\": {},\n",
    "                            \"df_exprs\": []\n",
    "                        }\n",
    "                    \n",
    "                    start_offset = match[\"start\"][\"offset\"]\n",
    "                    end_offset = match[\"end\"][\"offset\"]\n",
    "                    with open(file_path, \"r\") as f:\n",
    "                        content = f.read()\n",
    "                        snippet = content[start_offset:end_offset]\n",
    "                    \n",
    "                        if match[\"check_id\"] == \"pyspark-udf-definition\":\n",
    "                            udf_name = match[\"extra\"][\"message\"]\n",
    "                            file_dic[rel_path][\"udfs\"][udf_name] = {\n",
    "                                \"def\": snippet,\n",
    "                                \"calls\": []\n",
    "                            }\n",
    "\n",
    "                        elif match[\"check_id\"] == \"pyspark-df-expression\":\n",
    "                            # file_dic[rel_path][\"df_exprs\"].append(snippet)\n",
    "                            try:\n",
    "                                tree = ast.parse(snippet, mode=\"eval\")\n",
    "                                file_dic[rel_path][\"df_exprs\"].append(snippet)\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "                for match in matches:\n",
    "                    if match[\"check_id\"] != \"library-usage\":\n",
    "                        continue # only process library calls once udf's have been tagged\n",
    "\n",
    "                    msg_fields = match[\"extra\"][\"message\"].split(\":\", 2)\n",
    "                    func_name = msg_fields[0]\n",
    "                    library_name = msg_fields[1]\n",
    "                    call_name = msg_fields[2]\n",
    "\n",
    "                    file_path = match[\"path\"]\n",
    "                    rel_path = os.path.relpath(file_path, repo_dir)\n",
    "                    if rel_path in file_dic:\n",
    "                        if func_name in file_dic[rel_path][\"udfs\"]:\n",
    "                            file_dic[rel_path][\"udfs\"][func_name][\"calls\"].append({\n",
    "                                \"library\": library_name,\n",
    "                                \"call\": call_name\n",
    "                            })\n",
    "                \n",
    "                for file_data in file_dic.values():\n",
    "                    file_data[\"udfs\"] = [\n",
    "                        {\"name\": k, **v} for k,v in file_data[\"udfs\"].items()\n",
    "                    ]\n",
    "\n",
    "                repo_results[\"files\"] = [{\"path\": k, **v} for k,v in file_dic.items()]\n",
    "\n",
    "                # write to results jsonl\n",
    "                summary_file.write(json.dumps(repo_results, ensure_ascii=False) + \"\\n\")\n",
    "                summary_file.flush()\n",
    "\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Semgrep output not valid JSON.\")\n",
    "                print(semgrep_result.stdout[:500])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
