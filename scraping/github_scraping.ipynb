{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de39dcd",
   "metadata": {},
   "source": [
    "# Scraping GitHub for PySpark Usage\n",
    "\n",
    "This notebook provides a framework to clone repos and find pre-defined PySpark patterns defined in `./pyspark-rules.yml`. These patterns include PySpark DataFrame expressions, PySpark UDF definitions, and import usage in function definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a23cb47",
   "metadata": {},
   "source": [
    "## Using GitHub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6938728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from github import Github, Auth\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "if not GITHUB_TOKEN:\n",
    "    raise ValueError(\"Specify GITHUB_TOKEN in .env file.\")\n",
    "\n",
    "g = Github(auth=Auth.Token(GITHUB_TOKEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2328b7",
   "metadata": {},
   "source": [
    "## Currently looking at most popular repos mentioning PySpark\n",
    "\n",
    "Feel free to change the list of repos used for pattern searching. The most popular repos include a lot of tutorials and styleguides which may not be representative of true PySpark workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0683cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'AlexIoannides/pyspark-example-project', 'url': 'https://github.com/AlexIoannides/pyspark-example-project.git', 'stars': 2015, 'description': 'Implementing best practices for PySpark ETL jobs and applications.'}\n"
     ]
    }
   ],
   "source": [
    "query = \"pyspark in:name,description\"\n",
    "sort = \"stars\"\n",
    "order = \"desc\"\n",
    "limit = 1\n",
    "\n",
    "repos = g.search_repositories(query=query, sort=sort, order=order)\n",
    "\n",
    "popular_repos = []\n",
    "for i, repo in enumerate(repos):\n",
    "    if i >= limit:\n",
    "        break\n",
    "\n",
    "    popular_repos.append({\n",
    "        \"name\": repo.full_name,\n",
    "        \"url\": repo.clone_url,\n",
    "        \"stars\": repo.stargazers_count,\n",
    "        \"description\": repo.description\n",
    "    })\n",
    "\n",
    "for repo in popular_repos:\n",
    "    print(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1c9008",
   "metadata": {},
   "source": [
    "## Using Semgrep to search for PySpark patterns\n",
    "\n",
    "For each repo, the following cell will:\n",
    "1. Clone repo into a temp directory\n",
    "2. Convert any notebooks (`.ipynb`) into python files (`.py`) using `nbconvert`\n",
    "3. Capture the output of `semgrep scan` in a JSON object using the rules specified in `./pyspark-rules.yml`\n",
    "4. Process matches for DataFrame expressions and UDF definitions\n",
    "5. Process matches for imported library usage in functions that are tagged as UDFs\n",
    "6. Store processed result as JSON (see `./README.md` for schema)\n",
    "\n",
    "All results are stored in `./results/summary.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53ff7305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 potential matches in AlexIoannides/pyspark-example-project\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import git\n",
    "import json\n",
    "from nbconvert import PythonExporter\n",
    "import nbformat\n",
    "import tempfile\n",
    "import subprocess\n",
    "import ast\n",
    "\n",
    "# Create results jsonl\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "summary_path = os.path.join(\"results\", \"summary.jsonl\")\n",
    "\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as summary_file:\n",
    "    for repo in popular_repos:\n",
    "        repo_name = repo[\"name\"]\n",
    "        clone_url = repo[\"url\"]\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            # clone repo into temporary directory\n",
    "            repo_dir = os.path.join(tmpdir, \"repo\")\n",
    "            git.Repo.clone_from(clone_url, repo_dir, depth=1)\n",
    "\n",
    "            # find .ipynb files and convert into .py files\n",
    "            for root, _, files in os.walk(repo_dir):\n",
    "                for file in files:\n",
    "                    if file.endswith(\".ipynb\"):\n",
    "                        ipynb_path = os.path.join(root, file)\n",
    "                        py_path = os.path.join(root, \"CONVERTED\"+file.replace(\".ipynb\", \".py\"))\n",
    "\n",
    "                        with open(ipynb_path, \"r\", encoding=\"utf-8\") as ipynbf:\n",
    "                            nb_node = nbformat.read(ipynbf, as_version=4)\n",
    "\n",
    "                            exporter = PythonExporter()\n",
    "                            try:\n",
    "                                python_code, _ = exporter.from_notebook_node(nb_node)\n",
    "                                with open(py_path, \"w\", encoding=\"utf-8\") as pyf:\n",
    "                                    pyf.write(python_code)\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "\n",
    "            # use semgrep to detect udf definitions\n",
    "            semgrep_result = subprocess.run(\n",
    "                [\"semgrep\", \"scan\", \"--config\", \"pyspark-rules.yml\", repo_dir, \"--json\"],\n",
    "                capture_output=True,\n",
    "                encoding=\"utf-8\", #quick fix: to avoid byte serialization issue on Windows laptops.\n",
    "                text=True,\n",
    "                check=False\n",
    "            )\n",
    "\n",
    "            # parse pyspark dataframe expressions and track udf usage\n",
    "            try:\n",
    "                data = json.loads(semgrep_result.stdout)\n",
    "                matches = data.get(\"results\", [])\n",
    "                print(f\"Found {len(matches)} potential matches in {repo_name}\\n\")\n",
    "\n",
    "                repo_results = {\n",
    "                    \"repo_name\" : repo_name,\n",
    "                    \"clone_url\" : clone_url,\n",
    "                }\n",
    "                \n",
    "                file_dic = {}\n",
    "                for match in matches:\n",
    "                    if match[\"check_id\"] == \"library-usage\":\n",
    "                        continue # process dataframe expressions & udf definitions first\n",
    "\n",
    "                    file_path = match[\"path\"]\n",
    "                    rel_path = os.path.relpath(file_path, repo_dir)\n",
    "                    if rel_path not in file_dic:\n",
    "                        file_dic[rel_path] = {\n",
    "                            \"udfs\": {},\n",
    "                            \"df_exprs\": []\n",
    "                        }\n",
    "                    \n",
    "                    start_offset = match[\"start\"][\"offset\"]\n",
    "                    end_offset = match[\"end\"][\"offset\"]\n",
    "                    with open(file_path, \"r\") as f:\n",
    "                        content = f.read()\n",
    "                        snippet = content[start_offset:end_offset]\n",
    "                    \n",
    "                        if match[\"check_id\"] == \"pyspark-udf-definition\":\n",
    "                            udf_name = match[\"extra\"][\"message\"]\n",
    "                            file_dic[rel_path][\"udfs\"][udf_name] = {\n",
    "                                \"def\": snippet,\n",
    "                                \"calls\": []\n",
    "                            }\n",
    "\n",
    "                        elif match[\"check_id\"] == \"pyspark-df-expression\":\n",
    "                            # file_dic[rel_path][\"df_exprs\"].append(snippet)\n",
    "                            try:\n",
    "                                tree = ast.parse(snippet, mode=\"eval\")\n",
    "                                file_dic[rel_path][\"df_exprs\"].append(snippet)\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "                for match in matches:\n",
    "                    if match[\"check_id\"] != \"library-usage\":\n",
    "                        continue # only process library calls once udf's have been tagged\n",
    "\n",
    "                    msg_fields = match[\"extra\"][\"message\"].split(\":\", 2)\n",
    "                    func_name = msg_fields[0]\n",
    "                    library_name = msg_fields[1]\n",
    "                    call_name = msg_fields[2]\n",
    "\n",
    "                    file_path = match[\"path\"]\n",
    "                    rel_path = os.path.relpath(file_path, repo_dir)\n",
    "                    if rel_path in file_dic:\n",
    "                        if func_name in file_dic[rel_path][\"udfs\"]:\n",
    "                            file_dic[rel_path][\"udfs\"][func_name][\"calls\"].append({\n",
    "                                \"library\": library_name,\n",
    "                                \"call\": call_name\n",
    "                            })\n",
    "                \n",
    "                for file_data in file_dic.values():\n",
    "                    file_data[\"udfs\"] = [\n",
    "                        {\"name\": k, **v} for k,v in file_data[\"udfs\"].items()\n",
    "                    ]\n",
    "\n",
    "                repo_results[\"files\"] = [{\"path\": k, **v} for k,v in file_dic.items()]\n",
    "\n",
    "                # write to results jsonl\n",
    "                summary_file.write(json.dumps(repo_results, ensure_ascii=False) + \"\\n\")\n",
    "                summary_file.flush()\n",
    "\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Semgrep output not valid JSON.\")\n",
    "                print(semgrep_result.stdout[:500])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs230-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
